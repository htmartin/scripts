Introduction_to_Big_Data
June_2015

01_introduction_to_big_data

001_introduction_to_the_course.mp4
002_about_the_author.mp4

003_big_data_challenges.mp4

***What are the key sources for big data? (0:46)
transactions, social media (and whythese data suck for relational systems), unstructured data, sensor data

004_big_data_characteristics.mp4
volume, variety, veracity (determining if the data are correct), velocity

***How is analysis different for Big Data? (02:50)

integrating conventional and big data systems in your business

Businesses need to see a value from the big data chain - contribute to higher revenues
workflow needs to be executed in timely and cost-effective manner

005_problems_in_capitalizing_on_big_data.mp4
Pretty standard coverage of the issues a company will confront when moving to big data

006_solving_big_data_problems.mp4
New processing approaches, particularly MapReduce algorithm
dealing with datasets in memory (Hadoop)

***new data technologies that span data storage and processing

***Something about this "doing stuff in memory" versus other alternatives thing. 

***How stream processing changes the data workflow


007_the_challenges_of_relational_databases.mp4
volume/cost graph 
mostly linear for disks & processors
RDB systems costs do not behave linearly, very expensive as get larger. Also, going to have new kinds of data etc that RDBs can't store, work with etc

2factors:
how to take rdbs and make their relationship to cost linear? 
How to handle data rdbs can't handle?

systems this course will talk about address these issues


008_mapreduce_and_hadoop.mp4
starts with general steps/concepts involved in Processing Large volumes of data with many machines 

Basic foundation of Hadoop:
"what would we like to do with this dataset?""
- each expressed as a key value pair (goes with the load (L) step)
- map step (goes with the extract/transform (ET) step)
shuffle performed automatically in hadoop (goes with the shuffle step)
then reduce (goes with the aggregate step)

***The idea of key value pairs

009_mapreduce_algorithm.mp4
explains MR with simple example pets.txt file, same as in hadoop LP, this may be more clear

***Question on basic idea behind MR algorithm (actually would be good to have a few to use in different parts of the LP)

I don't quite get the point of the shuffle in his example
discusses something about shuffle and primary keys

03:00 Map step explained very basically w/o the master slave daemon etc
03:39 Reduce step

Separation of work b/t programmers and the MR Framework
beauty of hadoop is programmers can focus just on m and r, the framework handles a lot of important issues on its own, these are often difficult to program tasks

w/hadoop with don't move the data around (like you do in small data analysis?), you move the processing around - more efficient for big data



010_introducing_hadoop.mp4
open source framework for processing large amounts of data
design to work on cheap, unreliable machines that can fail anytime (so fault tolerance important)
designed to scale from the get go
big data for this course is more data than you are used to handling
usually hadoop combined with rdbs, nosql stores, etc to handle it

***Fault tolerance

***Hadoop = MR Framework plus HDFS
MR Framework 
- how hadoop undeerstands and assigns work to the nodes

HDFS
-where hadoop stores data
-a file system that spans all the nodes in a hadoop cluster
-links together the file systems on many local nodes to make them all into one big file system

data stored in redundant fashion, systems can be made self-healing

011_hadoop_distributed_file_system.mp4

how would you store data used for MR
- data on multiple machines
- should be able to store any kind of data
- problem: machines can fail, data should not be lost, how do we store the data on multiple machines
- solution
create distributed reliable storage

01:26 pictorial version of how hdfs works, master node, NNs etc
nn knows where blocks reside (on which machine)
shows replication picture
no big deal to use more space with replication because space is cheap
compares to raid drives, in hadoop use plain drives not raid

hdfs resides on top of native file systems (most often Linux)

block size is 64 or 128 MB most often

hdfs is open source verison of google file system (basically)

use smaller number of larger files (usu >100mb)

05:00 explains why ideal applications with hadoop read data from beginning to end (minimizes the cost of seek)

what this means is no random access (not sure exaclty what that means), but critters in hadoop ecosystem that can allow you to get around that

012_interacting_with_hdfs.mp4

hdfs shell is most common, gives standard form for commands

alternative: web user interface, all current hadoop distributions have these 

essential file movement commands:
get 
put
related commands: copyFromLocal and copyToLocal

other hdfs commands, resembles ordinary file system, will have usual file system commands (i.e., like linux)
- cat
- cp
- ls
- mkdir 
etc
***I guess this is why people keep calling hadoop the new OS for big data



013_hadoop_infrastructure.mp4

NN manages metadata for hdfs
NN must remain accessible (so more reliable machine for that)
metadata held in RAM, so NN needs a lot of it
Secondary NN
- this is not a backup!
DN
-stores data in standard files
- NN does something
- file split into blocks
- M & R jobs run on datanodes
***does make point that one way hadoop is different from traditional prcessing of data with databases is that you send the programs to the data instead of the reverse.

02:26
pic for how m and r tasks run on nodes
user programs are copied to all datanodes
who coordinates and keeps track of execution? that is JobTracker (which lives on master node)

hadoop management tools - early on fairly sparse, more coming now (not really much detail on what these are)

014_yarn.mp4

yarn is software part of modern hadoop infrastructure
came from trying to solve problems from Hadoop MR version 1

describes those problems

***add to yarn assessment - what was it about h1 that led to development of yarn?

hbase is the real time data (system?) on top of hadoop

resource management with YARN

***What can YARN do that couldn't be done before?
yarn is a common resource manage tool for application
gives
-scalability
-improved cluster utilization
-workloads other than MR

015_programming_hadoop.mp4

Things to think about
- you can't do updates in place
- your options vary for programming
	- full control vs productivity
	- developer background (programmer vs data person & level of experience with hadoop)

Java is an option
-low level api
-full control over all aspects of MR, because its written in java
-cumbersome programming model
- have to write tons of code (not efficient)
- explains why use Cascading (01:36)

Hadoop Streaming is another option
- typically used from languages like python or ruby
-simpler model than using java api
but less efficicient
- write m and r functions (system feeds data as input stream, writes as output stream)
- code shorter

***something about tradeoffs for choice of programming for hadoop (maybe for H LP+, see if explained well enough there to put it there)

Hive is another option
- sql like language
- much more productive than Java
- Hivesql is not as rich as regular sql
- but can extend it through user defined functions
so, good option for data professionals who know sql well, but a drawback is you may need to write a complex sql set of statements, so hard to write programs little by little

Pig is another option
- a data flow language
- allows you to build your programs out of small steps (so you can write a line, try it out, write another etc). use iterative and incremental approach to writing your programs
- much more productive than Java
- but designed to handle simple flows(so no control structures or iterations) 
- again, can extend it with user defined functions

Scalding is another option
- library built on top of Scala (a modern programming lang, descendant of Java, runs on JVM, can mix and match scalding and java)
- elegant model
- very short programs and easy to understand
- full programming environment so have full dev ecosystem (discusses what that gives you)

016_hive.mp4

(01:00) You aren't creating real tables. You are establishing a mapping between the files in hdfs and the tables that are logically used by the tables in HiveQL. i.e., (02:40) when you create a table, there is no separate storage allocated to that table, the data definition is just a mapping, Hive operates directly over the HDFS files.

Typically apply tools and processes similar to sql. 
e.g., process logs, texts, do indexing, ETL

Hive workflow
1. application dumps data into HDFS
2. Map data with Hive table definitions
3. Run queries
4. store results in HDFS
5. copy/move data from HDFS to other fs if need to use in further/conventional processing 

Internally, when running queries, hive creates needed mr jobs, (me: i.e., HiveQL written like sql but result is to create mr jobs, sort of translate between sql mind-set of processing data and the mr framework world it's actually running on)

Query language is a subset of sql

Problem with hive is that you're running a batch execution so can take a long time compared to other ways to program in hadoop


017_hive_architecture.mp4
1st slide: table comparing hive and relational databases - what they do and with what 

(03:00)
Talks about the metastore (this seems to be important for some reason)

To interact with, can use: hive, cli, web interface, language clients e.g., JDBC, python, ODBC


018_hive_data_model.mp4
Explains more about what the guy says in intro to hive about why this isn't really relational when talking about data model

Partitions help you structure larger datasets (each value of a partition key defines a partition of the data, can limit e.g., for files on a particular date or happened in a country). But this is not a real partition, the data aren't separated, this just limits the query etc to operate on data definied with those parameters - Speeds Things Up

Goes over hive data types

How to create a talbe, load data

Talks a lot about storing data in txt files etc (not sure how table is not really created, I guess just not created as a table, just a text file? is that replicated on different nodes? Does it exist just temporarily?, is this one of the results he discussed earlier that you then might export and do something else with? [probably the last at least])


019_hive_queries.mp4
I think the first slide here explains my question at the end of the last video.

Gives examples of user defined functions (types: simple that does one to one row mapping; UDaggreagage Function does many to one row mapping, UDTableFunction does one to many row mapping)

020_when_to_use_hive.mp4

when have large amounts of data
similarity with sql advantage
but limitations (no update, no single row INSERT)
Hive running on hadoop and this is for batch processing, so real-time is not best to do with that

021_pig.mp4
for processing semi-structured data

Pig latin
procedural - you write execution paths (scripts for e.g., filtering, joining, processing data)
scripting lang, not compiled
fairly high level, it converts scripts into mr jobs
user defined functions

more productive than hadoop with regular java

grunt is the pig interactive shell

022_pig_data_model.mp4

diff from relational model: model is nested

gives data types for simple (int, long.floats, doubles etc) and complex (fields, tuples, bags etc) data

Assign a schema to the data assigned "on the fly" or "on the read"

goes over loading, storing

023_pig_latin.mp4
data operations: filter, foreach, join, group - these are converted into mr flows



024_pig_example.mp4

025_when_to_use_pig.mp4

greta for parallel process of dimple data analysis tasks
we switch to pig when it seems to write a single query in Hive
(go iterative line by line development) bc can build programs in small steps to see at each step if program is working

much faster than java, but trade a lot less lines of code for a little longer to execute

pig is really good for raw data processing

Pig is not great when you need ...

026_scalding.mp4
027_programming_with_scalding.mp4
028_when_to_use_scalding.mp4

029_hadoop_ecosystem.mp4

goes over foundation is HDFS, yarn on top of that then pig hive tez hbase etc - see pic at start of slides

tez lets hive and pig work together (somehow) [talks about benefits in general]
hcatalog
mahout (scalable machine learning)
sqoop transfers data in parallel (rdb focused)
flume - distributed streaming for collegint acggregating and moving large amounts of log data, lots of data sources, many data outputs (hdfs, hbase etc), can use decorators to process data "in flight"
zookeeper is configuration management


030_hbase.mp4

real time DB that runs on top of hadoop, can do updates
columnar, distributed, has flexible schema (columns can be dynamically added), wide tables (columns into billoions), fault tolerant, inspired by BigTable, is a nosql system

transactions in hbase would work for a single row only, give up that for speed. [discusses other tradeoffs]

hbase data model is tables and rows

031_when_to_use_hbase.mp4
good for large amounts of data (over petabytes), when data model is simple and access pattern known in advance, schema doesn't need to be fixed (add columns dynamically)

don't use when need realtional processing with joins or small amounts of data

if hbase not good for you, consider Cassandra (and says why)

032_beyond_classic_hadoop_spark_and_flink.mp4

MR to Tez to Spark and Flink

Spark
resilient distributed datasets (datasets themselves can rebuild based on a memory store if failure occurs)
- very efficient

components: spark core engine at base, then spark SQL, Spark Streaming, MLLib, GraphX, SparkR

033_nosql_stores.mp4

NoSQL is not only sql (Me: looked up, did mean non SQL or non relational, the not only sql is a usage sometimes used)
is a non-relational data store

NOSQL is an umbrella name for various tech and approaches, chaning all the time, but in general, they are
non relational
distributed
horizontally scalable
no need for a fixed schema

(01:02) pic showing nosql impact on performance and cost

nosql - 1) solves want linear response to the increase in cost/performance ratio (instead of RDBs' crazy uptick as size of data increases), 2) want no limits on size of data itself (RDBs there is no way to do really big data)


***Question on nosql impact

scale up vs scale out - pic that shows the diff, basically adding better machines vs. add lots of crappy machines

talks about CAP theorem (04:00) and its implications

nosql stores
key-value
column family
doc oriented (json, xml)
graph db

decide based on requirements of your application 

***Question on diffs bt types of data structues used in nosql stores (maybe)


034_keyvalue_stores.mp4
035_columnar_stores.mp4
036_document_stores.mp4
037_graph_stores.mp4
038_data_modeling_for_nosql_stores.mp4

039_streaming.mp4
graphic at start

*** What is the main difference in working with streaming data (wording: vs batch jobs? batch data? batch stores?)?
Conventional processing, data are static, you send queries to data and get results out.
Real-time processing, the data are streaming, you send data at a queries store (is this correct?) and get results out

"passing data over a set of queries" which operate on some window of data, usually fairly small window

Streaming Uses: when need real-time advice bases on current set of data (or near real-time)
Personalization, search, revenue optimization, user events, content feeds, log processing, monitoring, recommendation

Storm, Spark Streaming, Flink, S4, Esper

040_storm.mp4
compares storm and hadoop (even though complement each other not compete)
what both do, what diffs are

Topology (conceptual organization): spouts are sources of streams, bolts consume them and emit tuples, flows of data bt spouts and bolts called streams (unbound sequence of tuples)

Physical organization (cluster set up or nodes set up) - pic at 01:30

Trident api
higher level abstraction than spouts and bolts
does stateful incrememntal processing on top of basic layer

compares (02:30) core and trident

best practices (aka tradeoffs in what to do)



041_spark_and_flink_streaming.mp4

spark
can use variety of inputs (flume, kafka, etc) pass data to spark streaming, then pass back out to HDFS, other Dbs or Dashboards

Spark Stream Processing
Input data goes into SStr, batches of input sent to Spark Engine, Spark Engine processes those data, then sends batches of processed data.
processeing micro batches of data collected over intervals of time

Flink, he talks about , but I'm not going to include Flink questions

042_lambda_architecture.mp4

provides solution to combining real time and batch data

pic on 1st slide shows how lambda architecture works
lambda architecture takes incoming data ans send to Event layer or Batch layer, stuff happens, then it merges results



043_introducing_big_data_and_nosql_in_the_enterprise.mp4

how to move from rdb to big data?
says start with quick win, Hadoop and Hive, then soon perhaps Pig

has a few diff problems (e.g., looking for more power?), try this...

***Question on transitions (covers systems, data stores, ETL, 04:29 - cloud,  etc)

reviews big data workflow from start

044_polyglot_persistence.mp4

addresses how to choose a db for a business application
PP is using a variety of DB stores in one application - might be best tech choice, can be operationally tough

generally try to minimize this variety, but know you could go PP if your system is pushed to limits

Reducing coupling- talks about designing for change

Data services: working with nosql, rest and cloud (I dont' really get why this is here)

045_seven_habits_of_successful_big_data_and_nosql_projects.mp4
046_wrapup.mp4





